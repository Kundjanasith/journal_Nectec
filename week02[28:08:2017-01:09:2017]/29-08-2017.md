# 29-08-2017

* Spark for Advanced Data Analysis
  * Real-Time Stream Processing
    * With a low-latency data analysis system at your disposal, it’s natural to extend the engine towards processing live data streams.
    *  Spark has an API for working with streams, providing exactly-once semantics and full recovery of stateful operators. It also has the distinct advantage of giving you the same Spark APIs to process your streams, including reuse of your regular Spark application code.
    * Spark Streaming API was released less than a year ago, users have deployed it in production to provide monitoring and alerting against stateful, aggregated data from system logs, achieving very fast processing with only seconds of latency.
  * Faster Decision-Making
    * One of the key properties of any decision is latency that is, the time it takes to make the decision from the moment the input data is available. Reducing decision latency can significantly increase their effectiveness, and ultimately increase the company’s return on investment. Since many of these decisions are based on complex computations (such as machine learning and statistical algorithms), Spark is an ideal fit to speed up decisions.
    * Spark has been deployed to improve decision quality as well as to reduce latency.
  * Unified Pipelines
    * MapReduce by integrating other frameworks for streaming, batch, and interactive computation. Users can dramatically reduce the complexity of their data processing pipelines by replacing several systems with Spark.
    * MapReduce to generate reports and answer historical queries, and deploy a separate system for stream processing to follow key metrics in real-time.
    * This approach requires one to maintain and manage two different systems, as well as develop applications for two different computation models. It would also require one to make sure the results provided by the two stacks are consistent.
    * Spark to implement stream processing as well as batch processing for providing historical reports.

* RDD
  * Spark started with only one data structure: the RDD which stands for Resilient Distributed Dataset. This data structure has interesting properties (as indicated by its name):
    * Resilient: A RDD is resilient and can be recomputed in case of failure
    * Distributed: A RDD can be partitioned and distributed over several nodes.
    * Immutable: A RDD can’t be modified. Instead we apply a transformation to generate a new RDD.
    * Lazy: A RDD represents a computation result but doesn’t trigger any computation. That means one can describe a whole chain of computation (or DAG – Directed Acyclic Graph) by transforming a RDD into another RDD.
    * Statically typed: A RDD has a type (e.g. RDD[String] or RDD[Person])

  * Dataframe
    * Dataframe was the second addition to Spark. It is a distributed collection of data organised into named columns.
    * The main drawbacks is the lack of type safety. Data frames have an associated schema representing the data. However the schema just holds the column names and not the column types. Therefore the user needs to cast the values to the expected type when accessing the content of a dataframe which lead to runtime errors.

  * Dataset
    * Dataset aims at adding type safety to the dataframes.
    * Datasets run on the SQL context and provide a syntax similar to RDDs with lambda expressions.

* Graphframe
  * One dataframe to store the graph vertices
  * One dataframe to store the graph edges
