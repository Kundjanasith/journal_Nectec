# 25-08-2017

* In-memory processing
  * In-memory processing is a developing technology for processing of data stored in an in-memory database.
  * Older systems have been based on disk storage and relational databases using SQL query language, but these are increasingly regarded as inadequate to meet business intelligence (BI) needs. Because stored data is accessed much more quickly when it is placed in Random Access Memory (RAM) or flash memory, in-memory processing allows data to be analyzed in real time, enabling faster reporting and decision-making in business.
  * Growing advantages of in-memory technology
    * Hardware becomes progressively cheaper and higher-performing, according to Mooreâ€™s law. Computing power doubles every two to three years while decreasing in costs. CPU processing, memory and disk storage are all subject to some variation of this law. Also hardware innovations such as multi-core architecture, NAND flash memory, parallel servers, and increased memory processing capability, in addition to software innovations such as column centric databases, compression techniques and handling aggregate tables, have all contributed to demand for in-memory products.
    * The advent of 64-bit operating systems, which allow access to far more RAM (up to 100GB or more) than the 2 or 4 GB accessible on 32-bit systems. By providing Terabytes (1 TB = 1,024 GB) of space for storage and analysis, 64-bit operating systems make in-memory processing scalable. The use of flash memory enables systems to scale to many Terabytes more economically.
    * Increasing volumes of data have meant that traditional data warehouses are no longer able to process the data in a timely and accurate way. The extract, transform, load (ETL) process that periodically updates data warehouses with operational data can take anywhere from a few hours to weeks to complete. So at any given point of time data is at least a day old. In-memory processing enables instant access to terabytes of data for real time reporting.
    * In-memory processing is available at a lower cost compared to traditional BI tools, and can be more easily deployed and maintained. According to Gartner survey, deploying traditional BI tools can take as long as 17 months. Many data warehouse vendors are choosing in-memory technology over traditional BI to speed up implementation times.

* Apache Spark
  * Apache Spark addresses these limitations by generalizing the MapReduce computation model, while dramatically improving performance and ease of use.

* Apache Hadoop
  * Apache Hadoop has revolutionized big data processing, enabling users to store and process huge amounts of data at very low costs.
  * MapReduce has proven to be an ideal platform to implement complex batch applications as diverse as sifting through system logs, running ETL, computing web indexes, and powering personal recommendation systems. However, its reliance on persistent storage to provide fault tolerance and its one-pass computation model make MapReduce a poor fit for low-latency applications and iterative computations, such as machine learning and graph algorithms.
